{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RUHAAN HAWALDAR BE 21137"
      ],
      "metadata": {
        "id": "lDHyP260xdGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
      ],
      "metadata": {
        "id": "X8lMKo22kJEA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_addition.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 500  // Number of elements\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "    int tid = threadIdx.x;\n",
        "    if (tid < N) {\n",
        "        c[tid] = a[tid] + b[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int a[N], b[N], c[N]; // Host arrays\n",
        "    int *dev_a, *dev_b, *dev_c; // Device pointers\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaError_t err = cudaMalloc((void**)&dev_a, N * sizeof(int));\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"Failed to allocate memory on device: %s\\n\", cudaGetErrorString(err));\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    cudaMalloc((void**)&dev_b, N * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_c, N * sizeof(int));\n",
        "\n",
        "    // Initialize host arrays\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        a[i] = i;\n",
        "        b[i] = i * i;\n",
        "    }\n",
        "\n",
        "    // CUDA Events for Timing\n",
        "    cudaEvent_t start, end;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&end);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Copy data from Host to Device\n",
        "    cudaMemcpy(dev_a, a, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch Kernel\n",
        "    add<<<1, N>>>(dev_a, dev_b, dev_c);\n",
        "\n",
        "    // Copy result back to Host\n",
        "    err = cudaMemcpy(c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"Failed to copy from device: %s\\n\", cudaGetErrorString(err));\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(end);\n",
        "    cudaEventSynchronize(end);\n",
        "\n",
        "    // Calculate Execution Time\n",
        "    float time = 0;\n",
        "    cudaEventElapsedTime(&time, start, end);\n",
        "    printf(\"Execution Time: %f ms\\n\", time);\n",
        "\n",
        "    // Print some results\n",
        "    for (int i = 0; i < 10; i++) { // Only print first 10 values\n",
        "        printf(\"%d + %d = %d\\n\", a[i], b[i], c[i]);\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoYC5QyQkl2G",
        "outputId": "96cf1fb9-1e55-41d3-ecb8-186d14d0c159"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vector_addition.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_addition.cu -o vector_addition\n",
        "!./vector_addition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Te5pcrLkrM0",
        "outputId": "ad1cda8a-1a08-4cb4-8544-7da264904c31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to allocate memory on device: CUDA driver version is insufficient for CUDA runtime version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./vector_addition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFYQyMP2ktIC",
        "outputId": "2ae8aa27-edfc-4613-b309-62086400fde0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Warning: unable to locate cuda driver library, GPU profiling skipped\n",
            "Failed to allocate memory on device: CUDA driver version is insufficient for CUDA runtime version\n",
            "======== Error: Application returned non-zero code 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDSdb3YmkvZ1",
        "outputId": "73978468-90c0-4cf3-c3ed-dd26bda96d33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vcm7K-Eky5f",
        "outputId": "38c9bc17-f6a7-4713-dfd4-b1bbc10f62e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmph5c083gf\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_multiplication.cu\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define m 10\n",
        "\n",
        "__global__ void mul_r(int *a, int *b, int *c){\n",
        "    int tid = threadIdx.x;\n",
        "    if (tid < m){\n",
        "        c[tid]= a[tid] * b[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int n, c, d, fst[10][10], snd[10][10], t_snd[10][10];\n",
        "    int row, col, sum_c, a[10], b[10], ans[10];\n",
        "\n",
        "    n = m;  // square matrix only\n",
        "\n",
        "    // Initialize first matrix with random values\n",
        "    for (c = 0; c < m; c++) {\n",
        "        for (d = 0; d < n; d++) {\n",
        "            fst[c][d] = rand() % 10 + 1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    printf(\"Elements of first matrix:\\n\");\n",
        "    for (c = 0; c < m; c++) {\n",
        "        for (d = 0; d < n; d++) {\n",
        "            printf(\"%d\\t\", fst[c][d]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Initialize second matrix with random values\n",
        "    for (c = 0; c < m; c++) {\n",
        "        for (d = 0; d < n; d++) {\n",
        "            snd[c][d] = rand() % 10 + 1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    printf(\"Elements of second matrix:\\n\");\n",
        "    for (c = 0; c < m; c++) {\n",
        "        for (d = 0; d < n; d++) {\n",
        "            printf(\"%d\\t\", snd[c][d]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Transpose of second matrix\n",
        "    for (c = 0; c < m; c++) {\n",
        "        for (d = 0; d < n; d++) {\n",
        "            t_snd[d][c] = snd[c][d];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    printf(\"\\nTranspose of second matrix:\\n\");\n",
        "    for (c = 0; c < n; c++) {\n",
        "        for (d = 0; d < m; d++) {\n",
        "            printf(\"%d\\t\", t_snd[c][d]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    int *dev_a, *dev_b, *dev_ans;\n",
        "    cudaMalloc((void**)&dev_a, m * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_b, m * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_ans, m * sizeof(int));\n",
        "\n",
        "    cudaEvent_t start, end;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&end);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (row = 0; row < m; row++) {\n",
        "        for (d = 0; d < m; d++) {\n",
        "            a[d] = fst[row][d];\n",
        "        }\n",
        "        cudaMemcpy(dev_a, a, m * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "        for (col = 0; col < m; col++) {\n",
        "            for (d = 0; d < m; d++) {\n",
        "                b[d] = t_snd[col][d];\n",
        "                ans[d] = 0;\n",
        "            }\n",
        "            cudaMemcpy(dev_b, b, m * sizeof(int), cudaMemcpyHostToDevice);\n",
        "            cudaMemcpy(dev_ans, ans, m * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "            mul_r<<<1, m>>>(dev_a, dev_b, dev_ans);\n",
        "            cudaMemcpy(ans, dev_ans, m * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "            sum_c = 0;\n",
        "            for (d = 0; d < m; d++) {\n",
        "                sum_c += ans[d];\n",
        "            }\n",
        "            snd[row][col] = sum_c;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(end);\n",
        "    cudaEventSynchronize(end);\n",
        "    float time = 0;\n",
        "    cudaEventElapsedTime(&time, start, end);\n",
        "    printf(\"Execution time=%f ms\\n\", time);\n",
        "\n",
        "    printf(\"Matrix multiplication result:\\n\");\n",
        "    for (c = 0; c < n; c++) {\n",
        "        for (d = 0; d < m; d++) {\n",
        "            printf(\"%d\\t\", snd[c][d]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_ans);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs8T-NmTkzgS",
        "outputId": "506be84e-44e3-4b95-ab40-0bdf00a781dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix_multiplication.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56jmriH-jzSS",
        "outputId": "07e8bcbf-3c39-42dd-bb4c-e2047151de48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elements of first matrix:\n",
            "4\t7\t8\t6\t4\t6\t7\t3\t10\t2\t\n",
            "3\t8\t1\t10\t4\t7\t1\t7\t3\t7\t\n",
            "2\t9\t8\t10\t3\t1\t3\t4\t8\t6\t\n",
            "10\t3\t3\t9\t10\t8\t4\t7\t2\t3\t\n",
            "10\t4\t2\t10\t5\t8\t9\t5\t6\t1\t\n",
            "4\t7\t2\t1\t7\t4\t3\t1\t7\t2\t\n",
            "6\t6\t5\t8\t7\t6\t7\t10\t4\t8\t\n",
            "5\t6\t3\t6\t5\t8\t5\t5\t4\t1\t\n",
            "8\t9\t7\t9\t9\t5\t4\t2\t5\t10\t\n",
            "3\t1\t7\t9\t10\t3\t7\t7\t5\t10\t\n",
            "Elements of second matrix:\n",
            "6\t1\t5\t9\t8\t2\t8\t3\t8\t3\t\n",
            "3\t7\t2\t1\t7\t2\t6\t10\t5\t10\t\n",
            "1\t10\t2\t8\t8\t2\t2\t6\t10\t8\t\n",
            "8\t7\t8\t4\t7\t6\t7\t4\t10\t5\t\n",
            "9\t2\t3\t10\t4\t10\t1\t9\t9\t6\t\n",
            "1\t10\t7\t4\t9\t6\t7\t2\t2\t6\t\n",
            "10\t9\t5\t9\t2\t1\t4\t1\t5\t5\t\n",
            "5\t5\t8\t7\t4\t2\t8\t6\t10\t7\t\n",
            "3\t2\t8\t9\t6\t8\t5\t2\t9\t6\t\n",
            "10\t8\t6\t4\t9\t9\t4\t2\t9\t10\t\n",
            "\n",
            "Transpose of second matrix:\n",
            "6\t3\t1\t8\t9\t1\t10\t5\t3\t10\t\n",
            "1\t7\t10\t7\t2\t10\t9\t5\t2\t8\t\n",
            "5\t2\t2\t8\t3\t7\t5\t8\t8\t6\t\n",
            "9\t1\t8\t4\t10\t4\t9\t7\t9\t4\t\n",
            "8\t7\t8\t7\t4\t9\t2\t4\t6\t9\t\n",
            "2\t2\t2\t6\t10\t6\t1\t2\t8\t9\t\n",
            "8\t6\t2\t7\t1\t7\t4\t8\t5\t4\t\n",
            "3\t10\t6\t4\t9\t2\t1\t6\t2\t2\t\n",
            "8\t5\t10\t10\t9\t2\t5\t10\t9\t9\t\n",
            "3\t10\t8\t5\t6\t6\t5\t7\t6\t10\t\n",
            "Execution time=0.000000 ms\n",
            "Matrix multiplication result:\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n"
          ]
        }
      ],
      "source": [
        "!nvcc -o matrix_multiplication matrix_multiplication.cu\n",
        "!./matrix_multiplication\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIUItshHlaVs"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}